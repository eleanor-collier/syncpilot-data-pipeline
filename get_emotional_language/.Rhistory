# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, sum(.)))
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
flatten()
View(corpusa.dtm)
sum(corpusa.dtm)
as.matrix(corpusa.dtm)
sum(as.matrix(corpusa.dtm))
test <- as.matrix(corpusa.dtm)
View(test)
View(TTR)
View(corpusa.dtm)
corpusa.dtm$v
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, sum(.$v)))
View(TTR)
TTR$corpusa.dtm[1]
TTR$corpusa.dtm[[1]]
TTR$corpusa.dtm[2]
TTR$corpusa.dtm[1]
View(TTR[[6]][[1]])
View(TTR[[6]][[2]])
TTR$corpusa.dtm[[1]]
TTR[$corpusa.dtm][6[[1]]
TTR[$corpusa.dtm[[6]][[1]]
TTR[[6]][[1]]
View(TTR[[6]][[1]])
TTR$corpusa.dtm$v
TTR$corpusa.dtm[[1]]$v
flatten(TTR$corpusa.dtm[[1]])
test <- flatten(TTR$corpusa.dtm[[1]])
View(test)
test$Docs
test$Terms
test$v
test <- flatten(TTR$corpusa.dtm)
View(test)
test <- flatten(TTR$corpusa.dtm)$v
test <- flatten(TTR$corpusa.dtm)
test$v
View(test)
test[[1]]$v
TTR$corpusa.dtm[[1]]$v
TTR$corpusa.dtm[1]
TTR$corpusa.dtm[1]$v
TTR$corpusa.dtm[[1]]$v
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, sum(.[[1]]]$v)))
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, sum(.[[1]]$v)))
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, sum(.[[1]])))
corpusa.dtm[[1]]
corpusa.dtm[[1]]$v
TTR$corpusa.dtm[[1]]$v
TTR$corpusa.dtm[[1]]
mutate(n_tokens = map(corpusa.dtm, sum(flatten(.)))
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, sum(flatten(.))))
TTR$corpusa.dtm[[1]]
flatten(TTR$corpusa.dtm[[1]])
flatten(TTR$corpusa.dtm[[1]])$v
test <- flatten(TTR$corpusa.dtm[[1]])
View(test)
sum(test)
TTR$corpusa.dtm[[1]]$v
sum(TTR$corpusa.dtm[[1]]$v)
TTR$corpusa.dtm[1]
TTR$corpusa.dtm[1][]
TTR$corpusa.dtm[1][]$v
TTR$corpusa.dtm[1]['v']
TTR$corpusa.dtm[[v]]
TTR$corpusa.dtm[['v']]
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = lmap(corpusa.dtm, sum((.$v))))
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = lmap(corpusa.dtm, ~sum((.$v))))
View(TTR[[6]][[1]])
View(TTR[[6]][[1]])
TTR$corpusa.dtm[[3]]
TTR$corpusa.dtm[[1]][[3]]
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, ~sum((.[[3]]))))
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, ~sum((.[[3]])))) %>%
mutate(n_types = map(corpusa.dtm, ~length(.$dimnames$Terms)))
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map(corpusa.dtm, ~sum((.[[3]])))) %>%
mutate(n_types = map(corpusa.dtm, ~length(.$dimnames$Terms))) %>%
mutate(TTR = n_types / n_tokens) %>%
select(TTR)
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map_dbl(corpusa.dtm, ~sum((.[[3]])))) %>%
mutate(n_types = map_dbl(corpusa.dtm, ~length(.$dimnames$Terms))) %>%
mutate(TTR = n_types / n_tokens) %>%
select(TTR)
View(TTR)
# Compute TTR for each transcript in the dataset
TTR <- transcripts %>%
select(ID, disclosure, text) %>%
mutate(corpus = map(text, ~Corpus(VectorSource(.)))) %>%
mutate(corpus.proc = map(corpus, ~tm_map(., FUN = tm_reduce, tmFuns = funcs))) %>%
mutate(corpusa.dtm = map(corpus.proc, ~DocumentTermMatrix(., control = list(wordLengths = c(3,10))))) %>%
mutate(n_tokens = map_dbl(corpusa.dtm, ~sum((.[[3]])))) %>%
mutate(n_types = map_dbl(corpusa.dtm, ~length(.$dimnames$Terms))) %>%
mutate(TTR = n_types / n_tokens) %>%
select(ID, disclosure, TTR)
View(TTR)
# Save data
write_csv(TTR, paste0(save_data_here, "type_token_ratio.csv"))
get_data_here  <- "/Users/Eleanor2/Library/CloudStorage/GoogleDrive-airfire246@gmail.com/My Drive/UCR/UCR SNL/Research Projects/SyncDisclosures/Pilot/Analysis/Data/processed/moment_to_moment/"
save_data_here <- "/Users/Eleanor2/Library/CloudStorage/GoogleDrive-airfire246@gmail.com/My Drive/UCR/UCR SNL/Research Projects/SyncDisclosures/Pilot/Analysis/Data/processed/mean_by_disclosure/"
#########################################################################################
### Load data ----
valence_ratings_neumdl <- read_csv(paste0(get_data_here, 'vocal_arousal_by_window_1s_neutral_modeling.csv'))
### Average arousal scores across disclosures ----
mean_arousal_ratings <- valence_ratings_neumdl %>%
group_by(ID, disclosure) %>%
summarize(
vocal_arousal = mean(arousal_score, na.rm=T),
vocal_arousal_max = max(arousal_score, na.rm=T),
vocal_arousal_sd = sd(arousal_score, na.rm=T),
vocal_arousal_span = max(arousal_score, na.rm=T) - min(arousal_score, na.rm=T),
) %>%
ungroup() %>%
mutate_all(~na_if(., -Inf)) %>%
get_data_here  <- "/Users/Eleanor2/Library/CloudStorage/GoogleDrive-airfire246@gmail.com/My Drive/UCR/UCR SNL/Research Projects/SyncDisclosures/Pilot/Analysis/Data/processed/moment_to_moment/"
save_data_here <- "/Users/Eleanor2/Library/CloudStorage/GoogleDrive-airfire246@gmail.com/My Drive/UCR/UCR SNL/Research Projects/SyncDisclosures/Pilot/Analysis/Data/processed/mean_by_disclosure/"
#########################################################################################
### Load data ----
valence_ratings_neumdl <- read_csv(paste0(get_data_here, 'vocal_arousal_by_window_1s_neutral_modeling.csv'))
### Average arousal scores across disclosures ----
mean_arousal_ratings <- valence_ratings_neumdl %>%
group_by(ID, disclosure) %>%
summarize(
vocal_arousal = mean(arousal_score, na.rm=T),
vocal_arousal_max = max(arousal_score, na.rm=T),
vocal_arousal_sd = sd(arousal_score, na.rm=T),
vocal_arousal_span = max(arousal_score, na.rm=T) - min(arousal_score, na.rm=T),
) %>%
ungroup() %>%
mutate_all(~na_if(., -Inf))
summary(mean_arousal_ratings)
# Save
write_csv(mean_arousal_ratings, paste0(save_data_here, 'vocal_arousal.csv'))
?nonindependence
??nonindependence
get_data_here  <- "/Users/Eleanor2/Library/CloudStorage/GoogleDrive-airfire246@gmail.com/My Drive/UCR/UCR SNL/Research Projects/SyncDisclosures/Pilot/Analysis/Data/processed/mean_by_disclosure/"
# Load data
baseline_raw <- read_csv(paste0(get_data_here, "empathic_accuracy.csv"))
get_data_here  <- "/Users/Eleanor2/Library/CloudStorage/GoogleDrive-airfire246@gmail.com/My Drive/UCR/UCR SNL/Research Projects/SyncDisclosures/Pilot/Analysis/Data/processed/mean_by_disclosure/"
# Load data
baseline_raw <- read_csv(paste0(get_data_here, "empathic_accuracy.csv"))
# Load data
empathic_accuracy <- read_csv(paste0(get_data_here, "empathic_accuracy.csv"))
View(empathic_accuracy)
dyadic_data <- empathic_accuracy %>%
group_by(ID) %>%
summarise(partner_empAcc = mean(partner_empAcc, na.rm=T))
View(dyadic_data)
# Load data
empathic_accuracy <- read_csv(paste0(get_data_here, "empathic_accuracy.csv")) %>%
group_by(ID) %>%
summarise(partner_empAcc = mean(partner_empAcc, na.rm=T)) %>%
ungroup()
### Assign pair numbers equal to odd subject number in each pair
dyadic_data <- empathic_accuracy %>%
## Assign pair numbers equal to odd subject number in each pair
mutate(pair_id = case_when(
ID %% 2 ==1 ~ ID, # odd
ID %% 2 ==0 ~ ID - 1 # even
)) %>%
# Switch order of subject numbers in pair in order to later combine with self-rating dataset
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup()
View(dyadic_data)
### Assign pair numbers equal to odd subject number in each pair
dyadic_data <- empathic_accuracy %>%
## Assign pair numbers equal to odd subject number in each pair
mutate(pair_id = case_when(
ID %% 2 ==1 ~ ID, # odd
ID %% 2 ==0 ~ ID - 1 # even
)) #%>%
### Assign pair numbers equal to odd subject number in each pair
dyadic_data <- empathic_accuracy %>%
## Assign pair numbers equal to odd subject number in each pair
mutate(pair_id = case_when(
ID %% 2 ==1 ~ ID, # odd
ID %% 2 ==0 ~ ID - 1 # even
)) #%>%
rename_with(~paste0("partner_", .)
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
. %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
rename_with(~paste0("partner_", .))
)
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
., . %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
rename_with(~paste0("partner_", .))
)
#########################################################################################
# Load data & get each participant's mean empathic accuracy
empathic_accuracy <- read_csv(paste0(get_data_here, "empathic_accuracy.csv")) %>%
group_by(ID) %>%
summarise(partner_empAcc = mean(partner_empAcc, na.rm=T)) %>%
ungroup() %>%
# Assign pair numbers equal to odd subject number in each pair
mutate(pair_id = case_when(
ID %% 2 ==1 ~ ID, # odd
ID %% 2 ==0 ~ ID - 1 # even
))
View(empathic_accuracy)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
rename_with(~paste0("partner_", .))
)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
. %>%
select(-pair_id) %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
rename_with(~paste0("partner_", .))
)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
dyadic_data %>%
select(-pair_id) %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
rename_with(~paste0("partner_", .))
)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(-pair_id) %>%
rename_with(~paste0("partner_", .))
)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
. %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(-pair_id) %>%
rename_with(~paste0("partner_", .))
)
dyadic_data
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(-pair_id) %>%
rename_with(~paste0("partner_", .))
)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair in order to combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) %>%
# Delete redundant rows
group_by(pair_ID) %>%
summarise_all(~first(.)) %>%
ungroup()
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) %>%
# Delete redundant rows
group_by(pair_id) %>%
summarise_all(~first(.)) %>%
ungroup()
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) %>%
# Delete redundant rows
group_by(pair_id) %>%
summarise_all(~first(.)) %>%
ungroup() %>%
select(pair_id, empAcc, partner_empAcc)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) %>%
# Delete redundant rows
group_by(pair_id) %>%
summarise_all(~first(.)) %>%
ungroup() %>%
select(pair_id, empAcc, partner_empAcc)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) %>%
# Delete redundant rows
group_by(pair_id) %>%
summarise_all(~first(.)) %>%
ungroup() #%>%
#########################################################################################
# Load data & get each participant's mean empathic accuracy
empathic_accuracy <- read_csv(paste0(get_data_here, "empathic_accuracy.csv")) %>%
group_by(ID) %>%
summarise(partner_empAcc = mean(partner_empAcc, na.rm=T)) %>%
ungroup() %>%
# Assign pair numbers equal to odd subject number in each pair
mutate(pair_id = case_when(
ID %% 2 ==1 ~ ID, # odd
ID %% 2 ==0 ~ ID - 1 # even
))
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) %>%
# Delete redundant rows
group_by(pair_id) %>%
summarise_all(~first(.)) %>%
ungroup() %>%
select(pair_id, empAcc, partner_empAcc)
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) #%>%
#########################################################################################
# Load data & get each participant's mean empathic accuracy
empathic_accuracy <- read_csv(paste0(get_data_here, "empathic_accuracy.csv")) %>%
group_by(ID) %>%
summarise(partner_empAcc = mean(partner_empAcc, na.rm=T)) %>%
ungroup() %>%
# Assign pair numbers equal to odd subject number in each pair
mutate(pair_id = case_when(
ID %% 2 ==1 ~ ID, # odd
ID %% 2 ==0 ~ ID - 1 # even
))
dyadic_data <- empathic_accuracy %>%
# Switch order of subject numbers in pair & combine with partner's data
bind_cols(
dyadic_data %>%
group_by(pair_id) %>%
arrange(pair_id, desc(ID)) %>%
ungroup() %>%
select(ID, partner_empAcc) %>%
rename(
partner_ID = ID,
empAcc = partner_empAcc
)
) #%>%
get_data_here  <- "/Users/Eleanor2/Library/CloudStorage/GoogleDrive-airfire246@gmail.com/My Drive/UCR/UCR SNL/Research Projects/SyncDisclosures/Pilot/Analysis/Data/processed/mean_by_disclosure/"
